\chapter{Conclusion}
\label{conclusion}

This chapter will first list future work, then conclude the thesis.

\section{Future work}

In this section we list future work and research in order of what we consider most
interesting to do more research on.

\subsection*{Type-3 clones}

Finding type-3 clones incrementally could be interesting to research. A classic algorithm
to detect type-3 algorithms is to use sparse dynamic programming to find matches of type-1
and type-2 clones which are not exactly similar~\cite{BakerSparseDynamicProgramming}. It
would be interesting to look at such an algorithm in an incremental perspective, and
determine if it would be beneficial to try and avoid computation of the same type-3 clones
in each revision. Another approach could be to create a dynamic data structure which is
simple to maintain between revisions, and allows extraction of type-3 clones in addition
to type-1 and type-2 clones. 


\subsection*{Optimal edit operations}

A problem we discussed in \cref{dynamicdetection} is that our algorithm for computing edit
operations is not optimal. The problem was to find an optimal amount of edit operations to
turn one string into another, but we also preferred edit operations of more than one
character, over many edit operations of one character. We implemented a simple algorithm
which would aggregate edit operations of the same type which were consecutive in the edit
matrix, but this could likely be optimized to output fewer operations with multiple
characters in each. There is a trade-off where one could always reduce the problem to only
two operations, delete the original string, and then insert the new string, but this might
be more expensive than performing more edit operations, with a fewer number of characters.
A possible approach is to assign a higher weight to starting a new edit operation than
extending the previous operation, and build the edit matrix with these weights in mind,
and using our algorithm to aggregate the edit operations afterwards with these weights.
This could be difficult if Hirschberg's algorithm is still used, but the regular
Wagner-Fischer algorithm could be used in a majority of cases if the memory usage of the
regular Wagner-Fischer matrix is not too big.

\subsection*{Refactoring of clones}

Another interesting topic is the refactoring of clones. Automatic refactoring of clones
has not been thoroughly researched, but automatic refactoring of code clones could be
beneficial to practically all software. Some literature exists on the
topic~\cite{TowardsAutomatedRefactoring, RefactoringOrientedClones}, especially targeted
at object-oriented programming languages. For CCDetect-LSP it would be interesting to look
at either combining the clone detection with existing refactoring tools from other LSP
clients, or to implement refactoring code-actions in CCDetect-LSP itself, which
automatically refactors to remove the selected clone. However, this would be hard to
achieve while CCDetect-LSP is still a language agnostic tool, as different languages have
completely different paradigms to perform refactoring on.

\subsection*{Compressing data structures}

A problem with CCDetect-LSP in its current state, is its memory usage. As mentioned, when
the size of the code base grows, the memory usage of CCDetect-LSP grows as well, and might
reach a point where it is not feasible for lower-end computers to run it simultaneously
with other applications, which is often the case in an IDE scenario. As mentioned, it is
possible that reducing the size of the fingerprint by fingerprinting entire lines instead
of tokens could improve the memory usage, but another potential solution is to compress
the memory consuming data structures. 

The wavelet matrix can be compressed to zero-order entropy, but Claude et al. explain that
this compressed wavelet matrix is likely not as efficient as a compressed wavelet
tree~\cite{WaveletMatrix}. It might be worthwhile to implement either a compressed wavelet
matrix or a compressed huffman-shaped wavelet tree, and test how this affects both memory
usage and performance. It is likely that the performance will be slower with this
compressed data structure, but the memory usage will likely improve as well.

It is also possible to compress the dynamic extended suffix array data structure as
explained by Salson et al.~\cite{DynamicExtendedSuffixArrays}. This is done by sampling
(storing) only some values of the dynamic permutation at a time, and computing the values
at unsampled positions by using the sampled values. This would again slow down the
performance of accessing the data structure, as computing the values at unsampled
positions is slower, but it will also reduce the number of values stored, and therefore
the memory usage will be improved.

\subsection*{Lazy LCP array updates}

In the suffix array update phase, for each edit operation we computed in the previous
phase, we run a suffix array update. In each one of these updates, we determine which
positions in the LCP array need to be updated, and then compute the LCP value at that
position. It is possible that we can avoid some work here if multiple of these edit
operations lead to updating the same LCP positions, as we can delay the computation of
those LCP values until after all the edit operations have updated the suffix array. This
would be of interest for algorithms which utilizes the dynamic extended suffix array
algorithm in general, if multiple edits are applied.

\section{Conclusion}

The results demonstrate that the incremental algorithm of CCDetect-LSP scales better than
the other two algorithms in terms of time, at least in the IDE scenario where edits are
relatively small. The incremental algorithm was able to process large edits for code bases
of over 5MLOC in under 2 seconds, and faster for smaller edits. We have demonstrated that
our novel application of dynamic extended suffix arrays is a suitable and fast approach to
clone detection in the IDE scenario, and can give comparable and better results than tools
which implement a dynamic suffix tree. Though memory consumption is the main limitation
for practical use, CCDetect-LSP still outperforms other incremental algorithms.

We have demonstrated that LSP can implement the bare-minimum features needed for a clone
detection tool, but some extensions would be useful in order to separate clone detection
from other types of diagnostics.

Finally, we have demonstrated that language agnostic clone detection using a parser
generator works and allows sufficient selection of fragments for flexible analysis, and to
avoid listing unwanted clones. We have also discussed that a language agnostic approach is
more challenging in terms of normalizing inputs and therefore detection of type-2 and
type-3 clones. However, this challenge was partly solved for type-2 clones by allowing the
client to configure how the input should be normalized.
