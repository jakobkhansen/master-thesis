\chapter{Conclusion and future work}

\section{Conclusion}

The results show that the incremental algorithm of CCDetect-LSP scales better than the
other two algorithms in terms of time, at least in the IDE scenario where edits are
relatively small. The algorithm was able to process large edits for code bases of over
5MLOC in under 2 seconds, and faster for smaller edits. We have shown that our novel
application of dynamic suffix arrays is a suitable and fast approach to clone detection,
and can give comparable and better results than tools which implement a dynamic suffix
tree. It also outperforms other incremental detection algorithms in terms of memory. Even
so, the memory usage is still high, and would preferably be lower when running on
computers with lower specifications, in an IDE scenario. 

We have shown that LSP can implement the bare-minimum features needed for a clone
detection tool, but some extensions would be useful in order to separate clone detection
from other types of diagnostics.

Finally, we have shown that language agnostic clone detection using a parser generator
works and allows sufficient selection of fragment selection to avoid listing unnecessary
clone information. We have also discussed that a language agnostic approach is more
challenging in terms of normalizing inputs and therefore detection of type-2 clones.

\section{Future work}

In this section we list future work and research in order of what we consider
most interesting to do more research on, to least.

\subsection*{Type-2 and type-3 clones}

An interesting topic which would benefit from further research is the topic of detecting
type-3 clones incrementally. A classic algorithm to detect type-3 algorithms is to use
sparse dynamic programming to find matches of type-1 and type-2 clones which are not
exactly similar~\cite{BakerSparseDynamicProgramming}. It would be interesting to look at
such an algorithm in an incremental perspective, and determine if it would be beneficial
to try and avoid computation of the same type-3 clones in each revision. Another approach
could be to create a dynamic data structure which is simple to maintain between revisions,
and allows extraction of type-3 clones in addition to type-1 and type-2 clones. 

Finding type-2 clones is also useful, but may not be of any interest for research, as
applying an input normalization of certain token types is likely a satisfactory solution
also for incremental detection.

\subsection*{Optimal edit operations}

A problem we discussed in \cref{dynamicdetection} is that our algorithm for computing edit
operations is not optimal. The problem was to find an optimal amount of edit operations to
turn one string into another, but we also preferred edit operations of more than one
character, over many edit operations of one character. We implemented a simple algorithm
which would aggregate edit operations of the same type which were consecutive in the edit
matrix, but this could likely be optimized. There is a trade-off where one could always
reduce the problem to only two operations, delete the original string, and then insert the
new string, but this might be more expensive than performing more edit operations, with a
fewer number of characters. A possible approach is to assign a higher weight to starting a
new edit operation than extending the previous operation, and build the edit matrix with
these weights in mind.

\subsection*{Refactoring of clones}

Another interesting topic is the refactoring of clones. Automatic refactoring of clones
has not been thoroughly researched, but automatic refactoring of code clones could be
beneficial to practically all software. Some literature exists on the
topic~\cite{TowardsAutomatedRefactoring, RefactoringOrientedClones}, especially targeted
at object-oriented programming languages. For CCDetect-LSP it would be interesting to look
at either combining the clone detection with existing refactoring tools from other LSP
clients, or to implement refactoring code-actions in CCDetect-LSP itself, which
automatically refactors to remove the selected clone. However, this would be a hard to
achieve while CCDetect-LSP is still a language agnostic tool, as different languages have
completely different paradigms to perform refactoring on.

\subsection*{Compressing data structures}

A problem with CCDetect-LSP in its current state, is its memory usage. As mentioned, when
the size of the code base grows, the memory usage of CCDetect-LSP grows as well, and might
reach a point where it is not feasible for lower-end computers to run it simultaneously
with other applications, which is often the case in an IDE scenario. As mentioned, it is
possible that reducing the size of the fingerprint by fingerprinting entire lines instead
of tokens could improve the memory usage, but another potential solution is to compress
the memory consuming data structures. 

The wavelet matrix can be compressed to zero-order entropy, but Claude et al. explain that
this compressed wavelet matrix is likely not as efficient as a compressed wavelet
tree~\cite{WaveletMatrix}. It might be worthwhile to implement either a compressed wavelet
matrix or a compressed huffman-shaped wavelet tree, and test how this affects both memory
usage and performance. It is likely that the performance will be slower with this
compressed data structure, but the memory usage will likely improve as well.

It is also possible to compress the dynamic extended suffix array data structure as
explained by Salson et al.~\cite{DynamicExtendedSuffixArrays}. This is done by sampling
(storing) only some values of the dynamic permutation at a time, and computing the values
at unsampled positions by using the sampled values. This would again slow down the
performance of accessing the data structure, as computing the values at unsampled
positions is slower, but it will also reduce the number of values stored, and therefore
the memory usage will be better.

\subsection*{Lazy LCP array updates}

In the suffix array update phase, for each edit operation we computed in the previous
phase, we run a suffix array update. In each one of these updates, we determine which
positions in the LCP array need to be updated, and then compute the LCP value at that
position. It is possible that we can avoid some work here if multiple of these edit
operations lead to updating the same LCP positions, as we can delay the computation of
those LCP values until after all the edit operations have updated the suffix array. We did
experiment with delaying the LCP update algorithm and processing the bitset which
contained the positions until after all the edit operations had updated the suffix array,
but we did not verify the correctness of this approach. This experiment did not show any
significant gain in performance, as it does seem that few LCP positions are updated
multiple times, but it is possible that our approach was not correct.

\section{Related work}

This section will list related work which have explored similar topics, or work which
could be useful in the previously listed future work.

Zibran et al. as previously cited, created the Eclipse plugin, called
SimEclipse\cite{Udding_Towards_Convenient_Management}. SimEclipse implements a suffix tree
based algorithm to find code clones of a selected fragment in source
code\cite{Zibran_real_time_search}. SimEclipse additionally implements many interesting
features, such as visualization of clones in a tree-view, simultaneous editing of clones,
and multiple features related to tracking the evolution of a clone.

Zhu et al. published a paper while this thesis was being written which shows a tool which
implements grammar-pluggable clone detection~\cite{GrammarPluggableCloneDetection}. This
is the same concept which we have implemented in CCDetect-LSP where we achieve language
agnostic clone detection by using a parser generator. They are able to identify all type-2
clones and has a $61\%$ recall for type-3 clones. Their algorithm seems to be based on a
completely different paradigm of clone detection than suffix trees and suffix arrays,
which could be interesting to examine for its potential in an incremental setting.
However, we should note that their evaluation shows that this type of clone detection is
not very fast, so it is not clear if this approach is suitable in a ``real-time'' setting.

For our dynamic extended suffix array implementation, we built upon the ideas of Salson et
al.~\cite{DynamicExtendedSuffixArrays, DynamicExtendedSuffixArraysReorderings, DynamicBWT}
There have been other attempts to improve such an algorithm, such as Amir et
al.~\cite{AmirDynamicSuffixWithPoly}. They claim to have obtained a sub-linear time for
queries in the suffix array, with a data structure where symbols can be substituted in the
input string, in $O(\log^4(n))$ time. This would not be suitable for our clone detection
algorithm, as we also require inserts and deletions. They also show a data structure with
$O(\log^5(n))$ query-time for the inverted suffix array, which can maintain the inverted
suffix array in $O(n^{\frac{2}{3}})$ time, and supports insertions and deletions as well.
It would be interesting to see if this data structure could in any way improve the
performance over our current approach, but this is questionable, as the data structure
needs to support querying the suffix array, inverted suffix array and the LCP array
efficiently, and preferably also support inserting/deleting multiple characters at a time
without performance problems.
