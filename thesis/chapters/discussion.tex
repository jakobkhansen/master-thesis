\chapter{Discussion}

This chapter will discuss the results of the different evaluations performed in
\cref{evaluation} and conclude with some insights into which algorithms seem to be more
efficient in certain cases. Which clones are detected will also be discussed, and the
possibility of extending the incremental algorithm of CCDetect-LSP to more intricate types
of clones.

\section{Performance}

In \cref{evaluation} we saw how the SACA detection, incremental detection and iClones
performed on multiple code bases of differing size and duplication. This section will
discuss these observations and consider which factors that contributed to the observed
outcomes and what this means for CCDetect-LSP as a clone detection tool in an IDE
environment. While we have tested iClones in our evaluation, we do not have access to the
source code, which is not open-source. Therefore, we cannot speak much to why iClones is
faster/slower on certain test instances.

It is clear that for the initial detection (revision 0), iClones has the best performance.
This was unexpected, as one would think that the SACA detection, which does not need to
build any extra data structures would be able to detect the clones faster than iClones.
However, we also see that the subsequent SACA detections (revisions 1-9) after the initial
detection is generally much faster than the initial iClones detection (revision 0). Since
much of the initial detection time is taken up by parsing the entire code base, it is
possible that the parsing in the SACA detection is slower than iClones parsing, not the
clone detection. Tree-sitter focuses on being performant on incremental re-parsing, and
therefore might not be as performant on the initial parse, which could explain why the
initial detection for both the CCDetect-LSP detections are slower than
iClones\footnote{Tree-sitter is not widely discussed in the literature, but is
open-source. This claim is based on the source code and discussions surrounding it.
(Github issues)}.

For the subsequent incremental detections (revisions 1-9), we see that depending on the
size of the code base, and the size of the edits, CCDetect-LSP incremental detection and
iClones are generally much faster than the SACA detection, as expected. For the smallest
code bases (WorldWind and neo4j), CCDetect-LSP incremental detection seems to be the
fastest for smaller edit sizes (INS10, DEL10), while iClones seems to be faster for the
larger edits (INS100, DEL100). It seems that when the code bases increase in size,
CCDetect-LSP incremental detection scales better than iClones and the SACA detection,
being able to handle incremental updates even for intellij-community (5.8MLOC) in less
than 1 second for smaller edits (INS10, DEL10). Elasticsearch (3.2MLOC) was the largest
code base that iClones could be run on with 16GB RAM, and for this code base it seems that
CCDetect-LSP has now started to outperform the other two detection algorithms. It would be
interesting to see if this trend continues for even larger code bases and compare
CCDetect-LSP incremental detection with iClones, but this would require a lot of memory to
benchmark.

In terms of stability, it seems that the SACA detection is naturally very stable in
running time between each revision, as it is recomputing the entire suffix array anyway.
The two incremental algorithms are more unstable in terms of running time, and this stems
from the fact that some incremental updates are easier than others. While we cannot speak
for the implementation of iClones, our incremental algorithms running time fluctuates
based mostly on how much time updating the suffix array takes. The time to update the
suffix array is mostly dependent on how many suffixes need to be reordered. The number of
reorderings directly affects how many LCP values need to be updated, which is often the
bottleneck of the suffix array update. In our testing we examined two different code bases
of size $\sim2.2\text{MLOC}$, graal and flink). We hypothesized that graal would have
slower updates, as it had a much higher $\text{LCP}_\text{avg}$, which would on average
lead to more reorderings. However, our results show that the running time for updates in
these code bases are very similar. The reason why we do not see much of a difference is
likely because the $\text{LCP}_\text{avg}$ differences in our code bases is too small to
see any noticeable effect by doubling the $\text{LCP}_\text{avg}$. Inspecting the numbers
in Salson et al.\cite{DynamicExtendedSuffixArraysReorderings} shows that the number of
reorderings for lower $\text{LCP}_\text{avg}$ values were often below $20$ and that the
number of reorderings only drastically increases for $\text{LCP}_\text{avg}$ in the
hundreds, which is more realistic for DNA sequences than for our source code fingerprints.


% 43 18
\section{Clones}
