\chapter{Discussion}

This chapter will discuss the results of the different evaluations performed in
\cref{evaluation} and conclude with some insights into which algorithms seem to be more
efficient in certain cases. Which clones are detected will also be discussed, and the
possibility of extending the incremental algorithm of CCDetect-LSP to more intricate types
of clones.

\section{Performance}

In \cref{evaluation} we saw how the SACA detection, incremental detection and iClones
performed on multiple code bases of differing size and duplication. This section will
discuss these observations and consider which factors that contributed to the observed
outcomes and what this means for CCDetect-LSP as a clone detection tool in an IDE
environment. While we have tested iClones in our evaluation, we do not have access to the
source code, which is not open-source. Therefore, we cannot speak much to why iClones is
faster/slower on certain test instances.

It is clear that for the initial detection (revision 0), iClones has the best performance.
This was unexpected, as one would think that the SACA detection, which does not need to
build any extra data structures would be able to detect the clones faster than iClones.
However, we also see that the subsequent SACA detections (revisions 1-9) after the initial
detection is generally much faster than the initial iClones detection (revision 0). Since
much of the initial detection time is taken up by parsing the entire code base, it is
possible that the parsing in the SACA detection is slower than iClones parsing, not the
clone detection. Tree-sitter focuses on being performant on incremental re-parsing, and
therefore might not be as performant on the initial parse, which could explain why the
initial detection for both the CCDetect-LSP detections are slower than
iClones\footnote{Tree-sitter is not widely discussed in the literature, but is
open-source. This claim is based on the source code and discussions surrounding it.
(Github issues)}.

For the subsequent incremental detections (revisions 1-9), we see that depending on the
size of the code base, and the size of the edits, CCDetect-LSP incremental detection and
iClones are generally much faster than the SACA detection, as expected. For the smallest
code bases (WorldWind and neo4j), CCDetect-LSP incremental detection seems to be the
fastest for smaller edit sizes (INS10, DEL10), while iClones seems to be faster for the
larger edits (INS100, DEL100). It seems that when the code bases increase in size,
CCDetect-LSP incremental detection scales better than iClones and the SACA detection,
being able to handle incremental updates even for intellij-community (5.8MLOC) in less
than 1 second for smaller edits (INS10, DEL10). Elasticsearch (3.2MLOC) was the largest
code base that iClones could be run on with 16GB RAM, and for this code base it seems that
CCDetect-LSP has now started to outperform the other two detection algorithms. It would be
interesting to see if this trend continues for even larger code bases and compare
CCDetect-LSP incremental detection with iClones, but this would require a lot of memory to
benchmark.

In terms of stability, it seems that the SACA detection is naturally very stable in
running time between each revision, as it is recomputing the entire suffix array anyway.
The two incremental algorithms are more unstable in terms of running time, and this stems
from the fact that some incremental updates are easier than others. While we cannot speak
for the implementation of iClones, our incremental algorithms running time fluctuates
based mostly on how much time updating the suffix array takes. The time to update the
suffix array is mostly dependent on how many suffixes need to be reordered. The number of
reorderings directly affects how many LCP values need to be updated, which is often the
bottleneck of the suffix array update. In our testing we examined two different code bases
of size $\sim2.2\text{MLOC}$, graal and flink). We hypothesized that graal would have
slower updates, as it had a much higher $\text{LCP}_\text{avg}$, which would on average
lead to more reorderings. However, our results show that the running time for updates in
these code bases are very similar. The reason why we do not see much of a difference is
likely because the $\text{LCP}_\text{avg}$ differences in our code bases is too small to
see any noticeable effect by doubling the $\text{LCP}_\text{avg}$. Inspecting the numbers
in Salson et al.\cite{DynamicExtendedSuffixArraysReorderings} shows that the number of
reorderings for lower $\text{LCP}_\text{avg}$ values were often below $20$ and that the
number of reorderings only drastically increases for $\text{LCP}_\text{avg}$ in the
hundreds, which is more realistic for DNA sequences than for our source code fingerprints.

\section{Clones}

In \cref{evaluation} we showed that CCDetect-LSP correctly identifies type-1 clones in the
BigCloneBench dataset. The results showed that CCDetect-LSP was able to identify
practically all type-1 clones, which gives us confidence that our algorithm is correct in
terms of the reported clones. BigCloneBench also reported that CCDetect-LSP was able to
detect some type-2 clones, but as previously stated, this is likely accidental.

Detecting type-1 clones is likely the most important clones to detect in an IDE scenario,
but type-2 clones are also likely useful. Recall that type-2 clones are clones which are
structurally identical, but allows differences in literals, identifiers and types. Recall
also that both type-1 and type-2 clones are clones which are often good targets for
refactoring, since they can be parameterized to account for the differences in literals
and types, before they are merged into a single function/method. In the case of
CCDetect-LSP, we could normalize the input to allow detection of type-2 clones by
consistently fingerprinting tokens to the same value if they are allowed to be different.
Because CCDetect-LSP is language agnostic and relies only on a Tree-sitter grammar for a
given programming language, performing this normalization is more challenging than in
typical clone detectors. This is because CCDetect-LSP needs to know which type of tokens
should be consistently fingerprinted as the same value. One might also want to only detect
clones with consistently substituted parameters, Baker's technique needs to be
implemented\cite{bakerdup} needs to be implemented as well. This could be done by allowing
the user to configure a list of AST node types which should be consistently fingerprinted,
but this also requires the user to be intimately familiar with the node types in the AST
of their chosen programming language.

For type-3 clones we need to consider how useful it is to report these clones in an IDE
scenario. Recall that type-3 clones are clones which in addition to type-1 and type-2
clones, allow some leniency in terms of the code clones structure. Type-3 clones allows
tokens to be added, modified or removed and two type-3 clones are considered equal based
on some similarity threshold. Seeing such clones in the IDE could possibly lead to a lot
of noise, and CCDetect-LSP could possibly detect a lot of clones which are not necessarily
simple to refactor and remove. It would be interesting to see how many type-3 clones are
reported in different code bases and how much value it would provide to the user to list
them in the IDE. It would also be interesting to see how implementing a type-3 clone
algorithm such as \cite{bakertype-3} would affect the speed of the incremental detection,
and if any type-3 detection algorithms can be made more efficient for an incremental
detection approach.

\Todo{Baker type-3 algorithm cite here}
