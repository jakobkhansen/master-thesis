\chapter{Discussion}
\label{discussion}

This chapter discusses the results of the evaluations performed in \cref{evaluation} and
conclude with some insights into which algorithms seem to be more efficient in each case.
We will also discuss the implications of the memory usage, and discuss the clones which
are detected, and the possibility of extending the incremental algorithm of CCDetect-LSP
to more intricate types of clones. Finally, this chapter will discuss some related work.

\section{Performance}

In \cref{evaluation} we saw how the SACA detection, incremental detection and iClones
performed on multiple code bases of differing size. This section will discuss these
observations and consider which factors that contributed to the observed outcomes and what
this means for CCDetect-LSP as a clone detection tool in an IDE environment. While we have
tested iClones in our evaluation, we do not have access to the source code. Therefore, we
cannot speak much to why iClones is faster/slower on certain test instances. It is
possible that iClones spends some time to build data structures for detecting type-3
clones, which would be unfair for the evaluation as type-3 detection is not considered.
However, it seems most of the work is avoided when type-3 clone detection is disabled, as
enabling it drastically slows down the benchmarks of iClones.

It is clear that for the initial detection (revision 0), iClones has the best performance.
This was unexpected, as one would think that the SACA detection, which does not need to
build any significant data structures, would be able to detect the clones faster than
iClones. However, we also see that the subsequent SACA detections (revisions 1-9) after
the initial detection is generally much faster than the initial iClones detection
(revision 0). Since much of the initial detection time is taken up by parsing the entire
code base, it is possible that the parsing in the SACA detection is slower than iClones
parsing, not the clone detection. Tree-sitter focuses on being performant on incremental
parsing, and therefore might not be as performant on the initial parse, which could
explain why the initial detection for both the CCDetect-LSP detections are slower than
iClones\footnote{Tree-sitter is not widely discussed in the literature, but is
open-source. This claim is based on the source code and discussions surrounding it, e.g.
Github issues.}. In the case of our incremental algorithm, it is not unexpected that the
initial detection is slower than the SACA initial detection, and therefore also the
slowest algorithm overall for the initial detection. Because our incremental algorithm
does the same work as the initial detection, but also needs to build the wavelet-matrix
and dynamic extended suffix array data structures, it seems to consistently be the slowest
algorithm in the initial detection. This is one of the tradeoffs of our incremental
algorithm, as the initial detection can be very slow for larger code bases, taking 3-4
minutes for the intellij-community code base. An interesting idea is to persist some of
the data on the disk, in order to avoid the slow initial detection of CCDetect-LSP. If the
fingerprint of each document was stored in a file on disk, this file could be read and
used to index the project without needing to parse every file before the detection can
happen. However, we encounter a problem whenever we want to perform an incremental update,
as we do not know which tokens map to which integer values in the fingerprint. This could
be solved by also persisting the token to integer mapping.

For the subsequent incremental detections (revisions 1-9), we see that depending on the
size of the code base, and the size of the edits, our incremental detection and iClones
are generally much faster than the SACA detection, as expected. For the smallest code
bases (WorldWind and neo4j), our incremental detection seems to be the fastest for smaller
edits (INS10, DEL10), while iClones seems to be faster for the larger edits (INS100,
DEL100). It seems that when the code bases increase in size, our incremental detection
scales better than iClones and the SACA detection. The incremental detection is able to
process updates to intellij-community (5.8MLOC) in less than 1 second for smaller edits
(INS10, DEL10) and less than 2 seconds for the larger edits (INS100, DEL100).
Elasticsearch (3.2MLOC) was the largest code base that iClones could be run on with 16GB
RAM, and for this code base it seems that our incremental detection starts to outperform
the other two detection algorithms in all cases. It would be interesting to see if this
trend continues for even larger code bases and continue to compare CCDetect-LSP
incremental detection with iClones, but this would require more RAM to benchmark. 

In the case of figure \ref{fig:elasticsearch_inc}, where we performed an evaluation of
elasticsearch and incresed the number of edits in each revision, we see that for a smaller
number of edits, our incremental detection has the best performance. For the smaller edits
(INS10, DEL10), the incremental detection outperforms or is comparable to the other
algorithms even up to 100 edits (1000 tokens inserted or deleted in total). However, we
see a clear trend that once the number of edits increase, our incremental algorithm is
slowly overtaken by iClones. For the larger edits (INS100, DEL100), we see that our
incremental detection is fastest for the smaller number of edits, but is quickly
outperformed by iClones and even the SACA detection at around 25 edits (2500 tokens
inserted or deleted in total). These results are an indication that CCDetect-LSP
incremental detection scales worse than the other algorithms when the number of edits
increase. Our incremental detection can outperform the other algorithms up to a decently
large number of edits, but if the code base drastically changes (thousands of tokens) in
each revision, iClones seems to have the best performance. Since editing thousands of
tokens at a time is unrealistic in the IDE scenario, we are therefore confident in saying
that CCDetect incremental detection still has the best performance for the IDE scenario,
but that this algorithm is not very suitable in the evolution scenario, if the changes in
each revision are affecting thousands of tokens.

In terms of stability of performance, it seems that the SACA detection is naturally very
stable between each revision, as it is recomputing the entire suffix array anyway. The two
incremental algorithms are more unstable in terms of running time, and this stems from the
fact that some incremental updates are less computationally heavy than others. We cannot
speak to the implementation of iClones, however, we often see that iClones takes an extra
revision or two before it stabilizes at a lower running time. We can also sometimes see
spikes in performance for iClones (such as in figure \ref{fig:neo4j}). Our incremental
algorithm has a running time which fluctuates mostly by how much time updating the suffix
array takes. The other phases are generally either very stable (source-mapping) or are too
fast to differentiate between revisions (parsing, fingerprinting). The time to update the
suffix array is highly dependent on how many suffixes needs to be reordered. The number of
suffixes being reordered directly affects how many LCP values need to be updated, which is
often the bottleneck of the suffix array update. In our testing we examined two different
code bases of size ${\sim}2.2\text{MLOC}$, graal and flink. We hypothesized that graal
would have slower updates, as its $\text{LCP}_\text{avg}$ was more than double that of
flink. According to Salson et al.~\cite{DynamicExtendedSuffixArraysReorderings}, this
should on average lead to more suffixes being reordered. However, our results show that
the running time for updates in these code bases are very similar. The reason why we do
not see much of a difference could be because the $\text{LCP}_\text{avg}$ differences in
our code bases is too small to see any noticeable effect by doubling the
$\text{LCP}_\text{avg}$. Inspecting the numbers by Salson et
al.~\cite{DynamicExtendedSuffixArraysReorderings}, shows that the number of suffixes
needing to be reordered for lower $\text{LCP}_\text{avg}$ values were often below $20$ and
that the number of reordered suffixes only drastically increases for
$\text{LCP}_\text{avg}$ in the hundreds. These values are more realistic for inputs such
as DNA sequences than for our source code fingerprints. It therefore seems that source
code is a good candidate to apply the dynamic suffix array update algorithm to, as updates
in source code will on average mean few suffixes need to be reordered.

With these results, it is clear that CCDetect-LSP incremental detection is fast enough to
be used in the IDE scenario for many code bases at least up to ${\sim}2.2$MLOC (flink,
graal), as most updates can be processed in under $1$ second for these code bases. For
code bases of larger sizes (elasticsearch, intellij), small edits (INS10, DEL10) can still
be processed in under $1$ second, but larger edits (INS100, DEL100) can start to take some
time, as the processing time exceeds $1$ second and can be closer to $2$ seconds. If the
programmer is generally only performing small edits in few files, and not doing
large-scale refactoring across many files at once, CCDetect-LSP is likely fast enough to
feel ``real-time'' even for these large code bases, but might feel sluggish to update on
such larger refactoring operations. It is also clear that while CCDetect-LSP incremental
detection is fast for the IDE scenario, it starts to get outperformed by other algorithms
in the evolution scenario if the number of edits increases, and the size of the edits get
larger.

\section{Memory usage}

In \cref{evaluation}, we also saw the results of the memory usage evaluation for the
different tools. This section will discuss these results and consider which factors
increase the memory usage of each tool, and what this means for CCDetect-LSP when used in
an IDE environment.

The results show a clear picture on how the memory usage scales with the number of lines
of code. The SACA detection has the lowest peak memory usage, which is expected, since it
only needs to store the extended suffix array in array form, which is more memory
efficient than the dynamic structures of the incremental algorithms. The main memory usage
for the SACA detection is the extended suffix array, which is not much larger than the
source-mapping information.

For the incremental algorithms it is clear that CCDetect-LSP incremental detection has a
lower memory usage than iClones, but the memory usage is still quite high compared to the
SACA detection. Inspecting the JProfiler memory overview shows that the main memory
bottleneck of CCDetect-LSP incremental detection is the wavelet-matrix, and the dynamic
extended suffix array data structures. These two data structures take up about the same
amount of memory, and since they both are pointer-based structures representing values for
the entire fingerprint, these two take up a lot of memory combined. For iClones, we see
that the memory usage is even more severe, seemingly doubling the memory usage of
CCDetect-LSP incremental detection. JProfiler reports that it is the suffix tree data
structure which is the memory bottleneck in iClones. 

While our incremental detection manages to lower its memory usage compared to iClones, the
memory usage is still quite high for an IDE tool. While the dynamic structure for storing
the extended suffix array gave us a large performance gain in terms of time, it is clear
that it is not nearly as memory efficient as the normal suffix array in array form. For a
larger code base such as elasticsearch and intellij-community, one would likely require a
computer with at least 16GB RAM in order to justify running CCDetect-LSP incremental
detection in the IDE. Again, it seems like CCDetect-LSP incremental detection is a good
fit for code bases up to ${\sim}2.2MLOC$ (graal, flink), where the memory usage is lower
than 4GB.

\section{Clones detected}

In \cref{evaluation} we saw that CCDetect-LSP identifies ${\sim}99.98\%$ of type-1 clones in
the BigCloneBench dataset. The results showed that CCDetect-LSP was able to identify
practically all type-1 clones, which gives us confidence that our algorithm is correct in
terms of the reported clones. BigCloneBench also reported that CCDetect-LSP was able to
detect some type-2 clones, but as previously stated, this is accidental.

We decided to store and represent clones as clone classes, rather than clone pairs. We
chose this because it gives a clearer picture to the programmer of all the clones of a
certain code snippet, rather than having to navigate multiple times between multiple
clones to determine all the clones of the snippet. Baars et al. also claims that storing
clone classes is advantageous in a refactoring scenario, which is intuitive, as
refactoring a set of clones likely leads to a better result than refactoring only a single
clone pair~\cite{TowardsAutomatedRefactoring}.

Detecting type-1 clones is likely the most important clones to detect in an IDE scenario,
but type-2 clones are also likely useful. Recall that type-2 clones are clones which are
structurally identical, but allows differences in literals, identifiers and types. Recall
also that both type-1 and type-2 clones are clones which are often good targets for
refactoring, since they can be parameterized to account for the differences in literals
and types, before they are merged into a single function/method. In the case of
CCDetect-LSP, we could normalize the input to allow detection of type-2 clones by
consistently fingerprinting tokens of the same type, but different value to the same value
if they are allowed to. Because CCDetect-LSP is language agnostic and relies only on a
Tree-sitter grammar for a given programming language, performing this normalization is
more challenging than in typical clone detectors, and would likely require the client to
send a list of token types which should be normalized. One might also want to
differentiate between consistently renaming tokens and consistently substituting specific
token values. An example of this is that one might only consider two snippets type-2
clones if their variable names are consistently renamed, meaning that there needs to be a
consistent mapping between variable names in the two snippets. Baker's technique could be
implemented to achieve this, but again, this seems difficult to achieve in a language
agnostic setting~\cite{Bakerdup}. This could be done by allowing the user to configure a
list of AST node types which should be consistently fingerprinted, but this also requires
the user to be intimately familiar with the node types in the AST of their chosen
programming language.

For type-3 clones we need to consider how useful it is to report these clones in an IDE
scenario. Recall that type-3 clones are clones which in addition to type-1 and type-2
clones, allow some leniency in terms of the code clones structure. Type-3 clones allows
tokens to be added, modified or removed and two type-3 clones are considered equal based
on some similarity threshold. Seeing such clones in the IDE could possibly lead to a lot
of noise, and CCDetect-LSP could possibly detect a lot of clones which are not necessarily
simple to refactor and remove. It would be interesting to see how many type-3 clones are
reported in large code bases and how much value it would provide to the user to list them
in the IDE. It would also be interesting to see how implementing a type-3 clone algorithm
such as Baker's algorithm~\cite{BakerSparseDynamicProgramming} would affect the speed of
the incremental detection, and if any type-3 detection algorithms can be made more
efficient for an incremental detection approach.

CCDetect-LSP identifies clones on a token level, unlike other tools which identifies
clones on a line level~\cite{Zibran_real_time_search}. It is more fine-grained to detect
clones on a token level, and it could detect clones which would not be detected on a line
level without needing to format the source code. However, it is possible that we could see
a substantial performance and memory usage gain by instead fingerprinting entire lines
instead of each token. This would drastically reduce the size of the fingerprint, which
would make the suffix array updates faster and reduce the size of the dynamic extended
suffix array and wavelet matrix. However, as there are a lot more unique lines in a code
base than tokens, this would also drastically increase the alphabet size, $\sigma$, which
will at least slow down operations on the wavelet matrix. This is a trade-off which would
be interesting to explore further.

\section{IDE and language agnostic clone detection}

In this section we will briefly discuss how well CCDetect-LSP works in both an IDE and
language agnostic setting.

\subsection*{IDE agnostic}

CCDetect-LSP was implemented as a standalone program, which communicates with an IDE
client via LSP. This means that the tool is IDE agnostic in the sense that any IDE which
implements LSP can communicate with CCDetect-LSP and receive code clone information from
it. With our implementation we are able to list clones in the form of diagnostics, and
navigate between all the matches of a clone via code-actions and/or information embedded
in the diagnostic. This demonstrates that LSP is capable of supporting the bare-minimum
needed functionality of a clone detection tool.

In the previous chapter, we discussed that CCDetect-LSP is able to run in at least two
IDEs which implement LSP, but we also saw that some LSP clients require more setup than
others, and some LSP clients implement fewer features. This is the reality of working with
a protocol which tries to unify multiple tools which do things differently. There will
always be some IDEs which do not implement the full LSP protocol, but we believe that the
features we have decided to use (diagnostics and code-actions) are good candidates to be
implemented in most IDEs. Barros et al.~\cite{LSPPractices} performed a study on LSP
servers, which showed that out of 30 LSP servers, 28 of them implemented diagnostics, and
15 of them implemented code-actions. While this is not necessarily an indication of what
the LSP clients implement, there is likely a correlation between what is generally offered
by LSP servers, and what is therefore supported by LSP clients.

One downside to reporting code clones as diagnostics is that code clones are listed
together with all other diagnostics reported by other LSP servers. If the project contains
hundreds of clones this could make the diagnostics list hard to use for any other usage
than to list clones. A nice addition to the LSP protocol in this regard would be the
possibility of categorizing diagnostics, and letting the client filter or show only
diagnostics of a certain category. For CCDetect-LSP it would be nice to be able to have a
separate view of only code clone diagnostics, which can be opened or closed as needed, and
allow another LSP server to be the ``main'' source of diagnostics.

Another interesting feature which could be implemented with LSP is the concept of linked
editing. Linked editing means being able to edit two or more locations at the same time,
which could be used as an automatic way to apply an edit to all clones at once. LSP has
since version 3.16 had such a feature, but we are not sure how fitting this feature is for
code clone linked editing, as it seems targeted towards smaller refactoring operations
such as renaming variables.

\subsection*{Language agnostic}

The algorithm of CCDetect-LSP is also language agnostic. We have shown that as long as
there exists a Tree-sitter grammar for a language,\footnote{Tree-sitter can parse most
mainstream languages, see: \url{https://tree-sitter.github.io/tree-sitter/}} it can be
parsed and fingerprinted. When the source code has been fingerprinted, there is no longer
any difference between detection for different languages. Therefore, CCDetect-LSP should
work for any language, and in our testing, there is nothing that hinders CCDetect-LSP from
working for any language we have tested, other than grammars not working correctly. We
have tested CCDetect-LSP for multiple languages, and shown that there is very little
configuration required to change the desired language. Therefore, we claim that
CCDetect-LSP works very well as a language agnostic code analysis tool.

In the case of the C language, which did not work correctly because of a quirk of its
grammar, it demonstrates a downside to going with the parser generator strategy. You
cannot completely rely on every grammar to always work as you expect. It could be possible
to forego parsing the source code, and only use a tokenizer to get the tokens. One could
then use a language agnostic tokenization algorithm to avoid needing a grammar at all, and
makes the algorithm truly language agnostic. However, this would not allow any sort of
fragment selection, which likely leads to a lot of unnecessary code being analyzed for
clones. This both slows down the detection process, but can also lead to a lot of noise in
the clone list. For example in Java, a sorted list of imports at the beginning of the file
could easily be cloned in multiple files, without any real possibility of being refactored
to avoid this duplication. For this reason, we believe a ``grammar-pluggable'' approach
with a parser generator such as Tree-sitter is a better approach than a completely
language agnostic one, for the IDE scenario.

\section{Related work}

This section will list related work which have explored similar topics, or work which
could be relevant for future work.

As mentioned in \cref{background}, the most common algorithm for clone detection is the
suffix-tree implementation. Therefore, many existing tools use a suffix
tree~\cite{GodeIncrementalCloneDetection, Zibran_real_time_search}. We have discussed the
disadvantages of the suffix tree in \cref{discussion}, mainly the memory usage. The tool
SimEclipse implements a suffix tree algorithm. Additionally, the tool implements many
interesting features, such as visualization of clones in a tree-view, simultaneous editing
of clones, and multiple features related to tracking the evolution of a clone.

Zhu et al. published a paper while this thesis was being written which shows a tool which
implements grammar-pluggable clone detection~\cite{GrammarPluggableCloneDetection}. This
is the same concept which we have implemented in CCDetect-LSP where we achieve language
agnostic clone detection by using a parser generator. They are able to identify all type-2
clones and has a $61\%$ recall for type-3 clones. Their algorithm seems to be based on a
completely different paradigm of clone detection than suffix trees and suffix arrays,
which could be interesting to examine for its potential in an incremental setting.
However, we should note that their evaluation shows that this type of clone detection is
not very fast, so it is not clear if this approach is suitable in a ``real-time'' setting.

While we are not aware of any other attempts at using dynamic suffix arrays for clone
detection, suffix arrays have been used for clone detection before. The tool SHINOBI uses
a suffix array index which can be queried for clones based on a string~\cite{SHINOBI}.
Their paper seems to indicate that only the edited code gets queried in the suffix array.

For our dynamic extended suffix array implementation, we built upon the ideas of Salson et
al.~\cite{DynamicExtendedSuffixArrays, DynamicExtendedSuffixArraysReorderings, DynamicBWT}
There have been other attempts to improve such an algorithm, such as Amir et
al.~\cite{AmirDynamicSuffixWithPoly}. They claim to have obtained a sub-linear time for
queries in the suffix array, with a data structure where symbols can be substituted in the
input string, in $O(\log^4(n))$ time. This would not be suitable for our clone detection
algorithm, as we also require inserts and deletions. They also show a data structure with
$O(\log^5(n))$ query-time for the inverted suffix array, which can maintain the inverted
suffix array in $O(n^{\frac{2}{3}})$ time, and supports insertions and deletions as well.
It would be interesting to see if this data structure could in any way improve the
performance over our current approach, but this is questionable, as the data structure
needs to support querying the suffix array, inverted suffix array and the LCP array
efficiently, and preferably also support inserting/deleting multiple characters at a time
without performance problems.
