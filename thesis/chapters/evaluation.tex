\chapter{Evaluation}


CCDetect will in this chapter be evaluated based on different criteria, which combined
will provide a basis for evaluating the tool as a whole.

Since the tool is focused on efficient detection of code clones, real-time performance of
the tool will be a high priority in its evaluation. Performance will be evaluated in
two ways:

\begin{itemize}
    \item Complexity analysis of phases in incremental detection
    \item Performance comparison of initial detection, incremental detection and another
        tool on code bases of different sizes
\end{itemize}

In addition, we will use the BigCloneBench~\cite{BigCloneBench} and
BigCloneEval~\cite{BigCloneEval} to verify that CCDetect-LSP correctly identifies type-1
clones.

\section{Time complexity of incremental detection}

In this section we will conduct an analysis of each phase of the incremental detection to
argue that the complexity of the incremental detection is more efficient than the initial
detection on average. In each phase we will argue the run time in terms of Big O notation.

For comparison's sake, the initial detection runs in $O(n)$ time where $n$ is the number of
characters in the code base. The bottleneck of the initial detection is reading and
parsing all the content in each file. Tree-sitter generates Generalized LR (GLR)
parsers~\cite{GLR}, which in the worst-case have a $O(n^3)$ running time, but $O(n)$ for
any deterministic grammar. As most programming languages have deterministic grammars, we
will assume that the running time of parsing with Tree-sitter takes $O(n)$ time.

\section{Performance comparison}

\Todo{Flame graphs?}

\section{Comparison with iClones}

\section{Memory usage}

\section{Verifying clones with BigCloneBench}
