\chapter{Evaluation}
\label{evaluation}

In this chapter, CCDetect-LSP will be evaluated based on different criteria, which combined
will provide a basis for evaluating the tool as a whole.

Since the tool is focused on efficient detection of code clones, real-time performance of
the tool will be a high priority in its evaluation. We will compare the time of the
initial detection with the incremental updates. Note that we will also distinguish between
the initial detection where parsing the entire code base is necessary, and subsequent
detections which still constructs the suffix array from scratch, but does not require
parsing the entire code base. We will call this type of detection the SACA detection,
while the detection which uses the dynamic extended suffix arrays will be called the
incremental detection. This is to show that the dynamic extended suffix array is faster
for this use case than building the suffix array from scratch, even when the initial
parsing is removed from the picture. Performance will be evaluated in two ways:

\begin{itemize}
    \item Complexity analysis of phases in initial detection and incremental detection
    \item Performance comparison of initial detection, incremental detection and another
        tool on code bases of different sizes
\end{itemize}

In addition, we will use the BigCloneBench~\cite{BigCloneBench} and
BigCloneEval~\cite{BigCloneEval} to verify that CCDetect-LSP correctly identifies type-1
clones.

\section{Time complexity of detection}

In this section we will conduct an informal analysis of the running time of each phase of
the initial, SACA and incremental detection to argue that the complexity of the
incremental detection is more efficient than the initial detection on average. In each
phase we will argue the run time in terms of Big O notation. Some claims will be
substantiated in the next section where we look at concrete code bases and their
properties.

The initial detection runs in $O(n)$ time, where $n$ is the number of characters in the
code base. The bottleneck of the initial detection is reading and parsing all the content
in each file. Tree-sitter generates Generalized LR (GLR) parsers~\cite{GLR}, which in the
worst-case have a $O(n^3)$ running time, but $O(n)$ for any deterministic grammar. As most
programming languages have deterministic grammars, we will assume that the running time of
parsing with Tree-sitter takes $O(n)$ time. After the initial parsing, the SACA detection
runs in $O(f)$ time, where $f$ is the size of the fingerprint and $f \ll n$. The running
time is $O(f)$ because the suffix array construction is performed for every update, which
takes linear time in the size of the input, which is the fingerprint. The extraction of
clones from the LCP array also runs in $O(f)$ as it is a single scan over the LCP array.
The final source-mapping is a bit more complicated, taking $O(\vert\text{clones}\vert
\times \log (\vert\text{documents}\vert))$, this is because for each clone, we binary
search the list of documents to find the correct document for that clone. This is highly
likely to be less time consuming than the suffix array construction, as the number of
documents and number of clones are usually multiple orders of magnitude lower than the
size of the whole code base. Therefore, we get a final running time of $O(f) +
O(\vert\text{clones}\vert \times \log(\vert\text{document}\vert))$, where $O(f)$ is highly
likely to be the factor which grows faster.

For the incremental detection, we have already parsed the code base and built the index
and dynamic extended suffix array structure for the code base. Afterwards, when an edit
$E$ is performed in a document $D$, extracting the edit operations takes $O(\vert D\vert +
\vert E\vert^2)$ where $\vert E\vert \leq \vert D\vert$. Note that the size of the edit is
calculated as the area which $E$ covers, meaning that if an edit consists of changing a
token at the beginning of the file, and a token at the end of the file, $\vert E\vert
\approx \vert D\vert$. Also note that $\vert D\vert$ is the size of the fingerprint for
the document $D$. We get this time complexity because Hirschberg's algorithm runs in $O(n
\times m)$ where in our case, $n \approx m$. If the size of the edit is contained in a
smaller area, we apply the optimization which reduces the size of the edit by comparing
characters at the beginning and end of the string, as discussed in
\cref{dynamicdetection}. This processes takes $O(\vert D\vert)$, and afterwards
Hirschberg's algorithm takes in worst-case $O(\vert E\vert^2)$, depending on how small the
previous optimization made the input strings.

The worst-case complexity of dynamically updating the extended suffix array is actually
slower than a linear time SACA algorithm in the worst-case. The worst-case scenario when
inserting/deleting a character in the fingerprint is that every single suffix needs to be
reordered, meaning we have $O(f)$ reorderings, where each reordering takes $O(\log(f))$
time, as it requires deleting and inserting an element in the dynamic extended suffix
array. This results in a $O(f \times \log(f))$ running time of this phase, which is worse
than the $O(f)$ running time of the SACA algorithm.

The average-case complexity of this phase is however highly likely to be faster. Salson et
al.~\cite{DynamicExtendedSuffixArraysReorderings} have shown that on average, the number
of reorderings required for an insertion/deletion in a suffix array is highly correlated
with the average LCP value of the input. Their data shows that for multiple different
types of data such as genome sequences and english text, the average LCP value of the
input is generally magnitudes lower than the input size. In our experience, this applies
to source code as well, as the average LCP values in the fingerprint of code bases we have
tested on have all had an average LCP values well below $100$. A lower number of
reorderings for lower LCP values seems intuitive, as lower LCP values mean that the
insertion/deletion will affect the ordering of fewer suffixes in the input. With this
information, it would be more accurate to downplay the importance of the $O(f)$ number of
reorderings in our analysis, and we therefore claim that the average running time of
inserting/deleting a character is closer to $O(\log(f))$. We extend this to account for
multiple insertions/deletions as well, so for each edit operations which was computed in
the last phase, we perform an insertion/deletion. Therefore, the total running time of
this phase on average is closer to $O(\vert\text{edits}\vert \times \log(f))$.

\Todo{Need to show average LCP value in performance benchmark to build on this claim.}

\Todo{Complexity of clone extraction and source-mapping in dynamic structures}

Next there is the clone extraction phase. Recall that in the dynamic detection clone
extraction phase, we had stored all nodes with an LCP value above the token threshold, and
iterate over those to determine which of them are clones or not. In the worst-case every
index in the LCP array would be above the token threshold, which would be an $O(f)$ time
traversal. However, this is highly unlikely, since the nodes with LCP value above the
token threshold are either clones, or contained clones. The number of contained clones is
limited by the number of clones because each clone can only have a limited amount of
contained clones. Therefore, the number of LCP nodes examined should be closer to
$O(\vert\text{clones}\vert)$, which is highly likely to be much less expensive than
iterating over all the LCP nodes. For each examined LCP node, we need to traverse from the
node, to its pointer node, and then to the root of the $B$ tree to determine the
fingerprint index of that LCP node. We do the same to determine the index of the matching
clone, as described in \cref{dynamicdetection}. These traversals take $O(\log f)$ time.
The worst-case performance of this phase is therefore $O(f \times \log(f))$, but we will
see that the average-case performance is closer to $O(\vert\text{clones}\vert \times
\log(f))$.

Finally, the source-mapping phase is easier to analyze. As we now know all the positions
of clones and their matches, building the clone-map takes $O(\vert\text{clones}\vert
\times \log(\vert\text{documents}\vert))$ time. We perform the binary-search over the
documents to get the source location, for each clone index we found in the previous phase.

With this informal analysis, we have shown that in the average case, the incremental
detection should be faster than the SACA detection, as none of the phases reach or exceed
the $O(f)$ running time of the SACA detection. In the next section we will show that the
properties we have assumed for this analysis are present in multiple code bases, and that
these properties do lead to better performance for the incremental detection.

\Todo{Statistic showing how many nodes above the token threshold}

\section{Benchmark performance}

\Todo{Introduce code bases taken from BigCloneBench}

\Todo{Flame graphs?}

\section{Comparison with iClones}

\section{Memory usage}

\section{Verifying clones with BigCloneBench}
