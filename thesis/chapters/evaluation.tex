\chapter{Evaluation}


CCDetect will in this chapter be evaluated based on different criteria, which combined
will provide a basis for evaluating the tool as a whole.

Since the tool is focused on efficient detection of code clones, real-time performance of
the tool will be a high priority in its evaluation. We will compare the time of the
initial detection with the incremental updates. Note that we will also distinguish between
the initial detection where parsing the entire code base is necessary, and subsequent
detections which still constructs the suffix array from scratch. We will call this type of
detection the SACA detection, while the detection which uses the dynamic extended suffix
arrays will be called the incremental detection. This is to show that the dynamic extended
suffix array is faster for this use case than building the suffix array from scratch.
Performance will be evaluated in two ways:

\begin{itemize}
    \item Complexity analysis of phases in incremental detection
    \item Performance comparison of initial detection, incremental detection and another
        tool on code bases of different sizes
\end{itemize}

In addition, we will use the BigCloneBench~\cite{BigCloneBench} and
BigCloneEval~\cite{BigCloneEval} to verify that CCDetect-LSP correctly identifies type-1
clones.

\section{Time complexity of detection}

In this section we will conduct an analysis of each phase of the initial, SACA and
incremental detection to argue that the complexity of the incremental detection is more
efficient than the initial detection on average. In each phase we will argue the run time
in terms of Big O notation.

The initial detection runs in $O(n)$ time where $n$ is the number of characters in the
code base. The bottleneck of the initial detection is reading and parsing all the content
in each file. Tree-sitter generates Generalized LR (GLR) parsers~\cite{GLR}, which in the
worst-case have a $O(n^3)$ running time, but $O(n)$ for any deterministic grammar. As most
programming languages have deterministic grammars, we will assume that the running time of
parsing with Tree-sitter takes $O(n)$ time. After the initial parsing, the SACA detection
runs in $O(f)$ time, where $f$ is the size of the fingerprint and $f \ll n$. This is
because the suffix array construction is performed for every update after the parsing,
which takes linear time. The extraction of clones from the LCP array also runs in $O(f)$
as it is a single scan over the LCP array. The final source-mapping is a bit more
complicated, taking $O(\vert\text{clones}\vert \times \log (\vert\text{documents}\vert))$,
this is because for each clone, we binary search the list of documents to find the correct
document for that clone. This is highly likely to be less time consuming than the suffix
array construction, as the number of documents and number of clones are usually multiple
orders of magnitude lower than the size of the whole code base.

For the incremental detection, we have already parsed the code base and built the index
and dynamic extended suffix array structure for the code base. Afterwards, when an edit
$E$ is performed in a document $D$, extracting the edit operations takes $O(\vert D\vert +
\vert E\vert^2)$ where $\vert E\vert \leq \vert D\vert$. Note that the size of the edit is
calculated as the area which $E$ covers, meaning that if an edit consists of changing a
token at the beginning of the file, and a token at the end of the file, $\vert E\vert
\approx \vert D\vert$. Also note that $\vert D\vert$ is the size of the fingerprint for
the document $D$. We get this time complexity because Hirschberg's algorithm runs in $O(n
\times m)$ where in our case, $n \approx m$. If the size of the edit is contained in a
smaller area, we apply the optimization which reduces the size of the edit by comparing
characters at the beginning and end of the string, as discussed in
\cref{dynamicdetection}. This processes takes $O(\vert D\vert)$, and afterwards
Hirschberg's algorithm takes $O(\vert E\vert^2)$.

\Todo{Complexity of extended suffix array updates with average case explained}
\Todo{Complexity of clone extraction and source-mapping in dynamic structures}

\section{Performance comparison}

\Todo{Flame graphs?}

\section{Comparison with iClones}

\section{Memory usage}

\section{Verifying clones with BigCloneBench}
