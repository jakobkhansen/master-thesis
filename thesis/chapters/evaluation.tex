\chapter{Evaluation}
\label{evaluation}

In this chapter, CCDetect-LSP will be evaluated based on different criteria, which combined
will provide a basis for evaluating the tool as a whole.

Since the tool is focused on efficient detection of code clones, real-time performance of
the tool will be a high priority in its evaluation. We will compare the time of the
initial detection with the incremental updates. Note that we will also distinguish between
the initial detection where parsing the entire code base is necessary, and subsequent
detections which still constructs the suffix array from scratch, but does not require
parsing the entire code base. We will call this type of detection the SACA detection,
while the detection which uses the dynamic extended suffix arrays will be called the
incremental detection. This is to show that the dynamic extended suffix array is faster
for this use case than building the suffix array from scratch, even when the initial
parsing is removed from the picture. Performance will be evaluated in two ways:

\begin{itemize}
    \item Complexity analysis of phases in initial detection and incremental detection
    \item Performance comparison of initial detection, incremental detection and another
        tool on code bases of different sizes
\end{itemize}

In addition, we will use the BigCloneBench~\cite{BigCloneBench} and
BigCloneEval~\cite{BigCloneEval} to verify that CCDetect-LSP correctly identifies type-1
clones.

\section{Time complexity of detection}

In this section we will conduct an informal analysis of the running time of each phase of
the initial, SACA and incremental detection to argue that the complexity of the
incremental detection is more efficient than the initial detection on average. In each
phase we will argue the run time in terms of Big O notation. Some claims will be
substantiated in the next section where we look at concrete code bases and their
properties.

The initial detection runs in $O(n)$ time, where $n$ is the number of characters in the
code base. The bottleneck of the initial detection is reading and parsing all the content
in each file. Tree-sitter generates Generalized LR (GLR) parsers~\cite{GLR}, which in the
worst-case have a $O(n^3)$ running time, but $O(n)$ for any deterministic grammar. As most
programming languages have deterministic grammars, we will assume that the running time of
parsing with Tree-sitter takes $O(n)$ time. After the initial parsing, the SACA detection
runs in $O(f)$ time, where $f$ is the size of the fingerprint and $f \ll n$. The running
time is $O(f)$ because the suffix array construction is performed for every update, which
takes linear time in the size of the input, which is the fingerprint. The extraction of
clones from the LCP array also runs in $O(f)$ as it is a single scan over the LCP array.
The final source-mapping is a bit more complicated, taking $O(\vert\text{clones}\vert
\times \log (\vert\text{documents}\vert))$, this is because for each clone, we binary
search the list of documents to find the correct document for that clone. This is highly
likely to be less time consuming than the suffix array construction, as the number of
documents and number of clones are usually multiple orders of magnitude lower than the
size of the whole code base. Therefore, we get a final running time of $O(f) +
O(\vert\text{clones}\vert \times \log(\vert\text{document}\vert))$, where $O(f)$ is highly
likely to be the factor which grows faster.

For the incremental detection, we have already parsed the code base and built the index
and dynamic extended suffix array structure for the code base. Afterwards, when an edit
$E$ is performed in a document $D$, extracting the edit operations takes $O(\vert D\vert +
\vert E\vert^2)$ where $\vert E\vert \leq \vert D\vert$. Note that the size of the edit is
calculated as the area which $E$ covers, meaning that if an edit consists of changing a
token at the beginning of the file, and a token at the end of the file, $\vert E\vert
\approx \vert D\vert$. Also note that $\vert D\vert$ is the size of the fingerprint for
the document $D$. We get this time complexity because Hirschberg's algorithm runs in $O(n
\times m)$ where in our case, $n \approx m$. If the size of the edit is contained in a
smaller area, we apply the optimization which reduces the size of the edit by comparing
characters at the beginning and end of the string, as discussed in
\cref{dynamicdetection}. This processes takes $O(\vert D\vert)$, and afterwards
Hirschberg's algorithm takes in worst-case $O(\vert E\vert^2)$, depending on how small the
previous optimization made the input strings.

The worst-case complexity of dynamically updating the extended suffix array is actually
slower than a linear time SACA algorithm in the worst-case. The worst-case scenario when
inserting/deleting a character in the fingerprint is that every single suffix needs to be
reordered, meaning we reorder $O(f)$ suffixes, where each reordering takes $O(\log(f))$
time, as it requires deleting and inserting an element in the dynamic extended suffix
array. In addition, we need to call the LF function twice for each reordering, which has a
$O(\sigma + \log n \log\sigma)$ complexity. The complexity of the LF function comes from a
$rank$ call in the wavelet matrix, and an iteration over $C$ to find the number of smaller
characters. This results in a $O(f \times (\log f + \sigma + \log n \log\sigma))$ running
time of this phase, which is worse than the $O(f)$ running time of the SACA algorithm. In
addition, before the reordering we use the LF function, insert/delete in the wavelet
matrix and the $C$ array. The highest complexity of these is again the LF function. If we
are inserting/deleting multiple characters, the number of LF function calls is increased,
with a complexity of $O(n \times (\sigma + \log n \log\sigma))$, where $n$ is the
number of characters inserted/deleted. Note that while there are many factors in this time
complexity, all the factors should be quite low compared to $f$, making the complexity of
reordering the suffixes the slowest factor of this phase.

The average-case complexity of this phase is however highly likely to be faster. Salson et
al.~\cite{DynamicExtendedSuffixArraysReorderings} have shown that on average, the number
of reorderings required for an insertion/deletion in a suffix array is highly correlated
with the average LCP value of the input. Their data shows that for multiple different
types of data such as genome sequences and english text, the average LCP value of the
input is generally magnitudes lower than the input size. In our experience, this applies
to source code as well, as the average LCP values in the fingerprint of code bases we have
tested on have all had an average LCP values well below $100$. A lower number of
reorderings for lower LCP values seems intuitive, as lower LCP values mean that the
insertion/deletion will affect the ordering of fewer suffixes in the input. With this
information, it would be more accurate to downplay the importance of the $O(f)$ number of
reorderings in our analysis, and we therefore claim that the average running time of
inserting/deleting a character is closer to $O(\log f + \sigma + \log n \log\sigma))$. We
extend this to account for multiple insertions/deletions as well, so for each edit
operations which was computed in the last phase, we perform an insertion/deletion.
Therefore, the total running time of this phase on average is closer to
$O(\vert\text{edits}\vert \times (\log f + \sigma + \log n \log\sigma)))$.

\Todo{Need to show average LCP value in performance benchmark to build on this claim.}

Next there is the clone extraction phase. Recall that in the dynamic detection clone
extraction phase, we had stored all nodes with an LCP value above the token threshold, and
iterate over those to determine which of them are clones or not. In the worst-case, every
index in the LCP array would be above the token threshold, which would be an $O(f)$
complexity iteration. However, this is highly unlikely, since the nodes with LCP value
above the token threshold are either clones, or contained clones. The number of contained
clones is limited by the number of clones because each clone can only have a limited
amount of contained clones. Therefore, the number of LCP nodes examined should be closer
to $O(\vert\text{clones}\vert)$, which is highly likely to be much less expensive than
iterating over all the LCP nodes. For each examined LCP node, we need to traverse from the
node, to its pointer node, and then to the root of the $B$ tree to determine the
fingerprint index of that LCP node. We do the same to determine the index of the matching
clone, as described in \cref{dynamicdetection}. These traversals take $O(\log f)$ time.
The worst-case performance of this phase is therefore $O(f \times \log(f))$, but we will
see that the average-case performance is closer to $O(\vert\text{clones}\vert \times
\log(f))$.

Finally, the source-mapping phase is easier to analyze. As we now know all the positions
of clones and their matches, building the clone-map takes $O(\vert\text{clones}\vert
\times \log(\vert\text{documents}\vert))$ time. We perform the binary-search over the
documents to get the source location, for each clone index we found in the previous phase.

With this informal analysis, we have shown that in the average case, the incremental
detection should be faster than the SACA detection, as none of the phases reach or exceed
the $O(f)$ running time of the SACA detection. In the next section we will show that the
properties we have assumed for this analysis are present in multiple code bases, and that
these properties do lead to better performance for the incremental detection.

\Todo{Statistic showing how many nodes above the token threshold}

\section{Benchmark performance}

\Todo{Introduce code bases taken from BigCloneBench}

\Todo{Flame graphs?}

\section{Comparison with iClones}

\section{Memory usage}

\section{Verifying clones with BigCloneBench}

In order to verify that CCDetect-LSP correctly identifies clones, we should also verify
that the clones the tool outputs are actually clones. BigCloneBench~\cite{BigCloneBench}
is a clone detection benchmark of a set of known clones in a dataset called
``IJaDataset''. The benchmark consists of the IJaDataset and a database containing
information of the clones which exist in the dataset. BigCloneEval~\cite{BigCloneEval} is
a tool which simplifies evaluation of a tool on the BigCloneBench. BigCloneEval evaluates
a tool by running the tool on the dataset, letting the tool output all the clones the tool
finds, and then matching the clones the tool found with the clones in the database. The
clones in the database are manually verified by humans, and include type-1 to type-3
clones.

We evaluated CCDetect-LSP by running the initial detection algorithm on the dataset, and
outputting all the clones the tool found. BigCloneEval expects to get a file where each
line contains a clone pair, where a clone is specified with filename, starting and
ending line. This was simple to extract from our clone-map, where we converted our ranges
which point to the beginning and ending token, to just the line number of those tokens.

BigCloneEval reports the recall of a tool, meaning the percentage of found clones.
Therefore, we did not make sure that our conversion to the format BigCloneEval expects
gave the minimum number of clone pairs. For example, for a clone pair of clones $A$ and
$B$, we output both that $A$ is a clone of $B$ and that $B$ is a clone of $A$, which is
superfluous. This would lead to a bad precision, but does not affect the recall in the
report which BigCloneEval outputs.

We used the following command and parameters to evaluate CCDetect-LSP:
$$
\text{./evaluateTool --min-tokens=100 -s=100 --output=ccdetect.report -t=1 }
$$

For CCDetect-LSP, we used mostly the default parameters of BigCloneEval, but we increased
the minimum clone size to $100$ and set the minimum similarity threshold to be $100\%$,
since we are only evaluating CCDetect-LSP for type-1 clone recall. This was done to reduce
the time an evaluation takes as it does not consider type-3 clones when the similarity
threshold is set to $100\%$, and the default token threshold of $50$ requires a lot of
time to match clones with the database.

\Todo{Now logs of bcb report}
