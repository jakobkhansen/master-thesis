@article{DynamicExtendedSuffixArrays,
	title = {Dynamic extended suffix arrays},
	journal = {Journal of Discrete Algorithms},
	volume = {8},
	number = {2},
	pages = {241-257},
	year = {2010},
	note = {Selected papers from the 3rd Algorithms and Complexity in Durham Workshop ACiD 2007},
	issn = {1570-8667},
	doi = {https://doi.org/10.1016/j.jda.2009.02.007},
	url = {https://www.sciencedirect.com/science/article/pii/S1570866709000343},
	author = {M. Salson and T. Lecroq and M. Léonard and L. Mouchard},
	keywords = {Dynamic, Suffix array, Edit operations, Algorithm design, Self-index data structures, FM-index},
	abstract = {The suffix tree data structure has been intensively described, studied and used in the eighties and nineties, its linear-time construction counterbalancing his space-consuming requirements. An equivalent data structure, the suffix array, has been described by Manber and Myers in 1990. This space-economical structure has been neglected during more than a decade, its construction being too slow. Since 2003, several linear-time suffix array construction algorithms have been proposed, and this structure has slowly replaced the suffix tree in many string processing problems. All these constructions are building the suffix array from the text, and any edit operation on the text leads to the construction of a brand new suffix array. In this article, we are presenting an algorithm that modifies the suffix array and the Longest Common Prefix (LCP) array when the text is edited (insertion, substitution or deletion of a letter or a factor). This algorithm is based on a recent four-stage algorithm developed for dynamic Burrows–Wheeler Transforms (BWT). For minimizing the space complexity, we are sampling the Suffix Array, a technique used in BWT-based compressed indexes. We furthermore explain how this technique can be adapted for maintaining a sample of the Extended Suffix Array, containing a sample of the Suffix Array, a sample of the Inverse Suffix Array and the whole LCP array. Our practical experiments show that it operates very well in practice, being quicker than the fastest suffix array construction algorithm.}
}

@article{DynamicExtendedSuffixArraysReorderings,
title = {On the number of elements to reorder when updating a suffix array},
journal = {Journal of Discrete Algorithms},
volume = {11},
pages = {87-99},
year = {2012},
note = {Special issue on Stringology, Bioinformatics and Algorithms},
issn = {1570-8667},
doi = {https://doi.org/10.1016/j.jda.2011.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570866711000037},
author = {M. Léonard and L. Mouchard and M. Salson},
keywords = {FM-index, Burrows–Wheeler Transform, Suffix array, Longest common prefix, Dynamic data structure, Self-index, Complexity},
abstract = {Recently new algorithms appeared for updating the Burrows–Wheeler Transform or the suffix array, when the text they index is modified. These algorithms proceed by reordering entries and the number of such reordered entries may be as high as the length of the text. However, in practice, these algorithms are faster for updating the Burrows–Wheeler Transform or the suffix array than the fastest reconstruction algorithms. In this article we focus on the number of elements to be reordered for real-life texts. We show that this number is related to LCP values and that, on average, Lave entries are reordered, where Lave denotes the average LCP value, defined as the average length of the longest common prefix between two consecutive sorted suffixes. Since we know little about the LCP distribution for real-life texts, we conduct experiments on a corpus that consists of DNA sequences and natural language texts. The results show that apart from texts containing large repetitions, the average LCP value is close to the one expected on a random text.}
}


@article{DynamicBWT,
	title = {A four-stage algorithm for updating a Burrows–Wheeler transform},
	journal = {Theoretical Computer Science},
	volume = {410},
	number = {43},
	pages = {4350-4359},
	year = {2009},
	note = {String Algorithmics: Dedicated to Professor Maxime Crochemore on the occasion of his 60th birthday},
	issn = {0304-3975},
	doi = {https://doi.org/10.1016/j.tcs.2009.07.016},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397509004770},
	author = {M. Salson and T. Lecroq and M. Léonard and L. Mouchard},
	keywords = {Burrows–Wheeler transform, Compression, Dynamic, Suffix array, Edit operations, Algorithm design, Self-index data structures},
	abstract = {We present a four-stage algorithm that updates the Burrows–Wheeler Transform of a text T, when this text is modified. The Burrows–Wheeler Transform is used by many text compression applications and some self-index data structures. It operates by reordering the letters of a text T to obtain a new text bwt(T) which can be better compressed. Even though recent advances are offering this structure new applications, a major bottleneck still exists: bwt(T) has to be entirely reconstructed from scratch whenever T is modified. We study how standard edit operations (insertion, deletion, substitution of a letter or a factor) that transform a text T into T′ impact bwt(T). Then we present an algorithm that directly converts bwt(T) into bwt(T′). Based on this algorithm, we also sketch a method for converting the suffix array of T into the suffix array of T′. We finally show, based on the experiments we conducted, that this algorithm, whose worst-case time complexity is O(|T|log|T|(1+logσ/loglog|T|)), performs really well in practice and replaces advantageously the traditional approach.}
}


@book{fowlerrefactoring,
	author    = {Martin Fowler},
	title     = {Refactoring - Improving the Design of Existing Code},
	series    = {Addison Wesley object technology series},
	publisher = {Addison-Wesley},
	year      = {1999},
	url       = {http://martinfowler.com/books/refactoring.html},
	isbn      = {978-0-201-48567-7},
	timestamp = {Wed, 25 Mar 2015 11:31:06 +0100},
	biburl    = {https://dblp.org/rec/books/daglib/0019908.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{Inoue_introduction_to_cc,
	author="Inoue, Katsuro",
	editor="Inoue, Katsuro
	and Roy, Chanchal K.",
	title="Introduction to Code Clone Analysis",
	bookTitle="Code Clone Analysis: Research, Tools, and Practices",
	year="2021",
	publisher="Springer Singapore",
	address="Singapore",
	pages="3--27",
	abstract="Code Clone is a code snippet that has the same or similar code snippet in the same or different software system. The existence of code clones is an issue on software maintenance and a clue to understanding the structure and evolution of software systems. A large number of researches on code clones have been performed, and many tools for code clone analysis have been developed. In this chapter, we will explain some of the terms that are important for understanding code clones, such as definition, type, analysis granularity, and analysis domain. We will also outline the approaches and applications of code clone analysis.",
	isbn="978-981-16-1927-4",
	doi="10.1007/978-981-16-1927-4_1",
	url="https://doi.org/10.1007/978-981-16-1927-4_1"
}


@incollection{BigCloneBench,
	author    = {Jeffrey Svajlenko and
			Chanchal K. Roy},
	editor    = {Katsuro Inoue and
			Chanchal K. Roy},
	title     = {{BigCloneBench}},
	booktitle = {Code Clone Analysis},
	pages     = {93--105},
	publisher = {Springer Singapore},
	year      = {2021},
	url       = {https://doi.org/10.1007/978-981-16-1927-4\_7},
	doi       = {10.1007/978-981-16-1927-4\_7},
	timestamp = {Sat, 21 Aug 2021 14:12:45 +0200},
	biburl    = {https://dblp.org/rec/books/sp/21/SvajlenkoR21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{BigCloneEval,
	author={Svajlenko, Jeffrey and Roy, Chanchal K.},
	booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
	title={BigCloneEval: A Clone Detection Tool Evaluation Framework with BigCloneBench},
	year={2016},
	volume={},
	number={},
	pages={596-600},
	doi={10.1109/ICSME.2016.62}
}

@inproceedings{Rohit_Gheyi_Impact,
author    = {Diego Cedrim and
Alessandro Garcia and
Melina Mongiovi and
Rohit Gheyi and
Leonardo da Silva Sousa and
Rafael Maiani de Mello and
Baldoino Fonseca and
M{\'{a}}rcio Ribeiro and
Alexander Ch{\'{a}}vez},
editor    = {Eric Bodden and
Wilhelm Sch{\"{a}}fer and
Arie van Deursen and
Andrea Zisman},
title     = {Understanding the impact of refactoring on smells: a longitudinal
		study of 23 software projects},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software
		Engineering, {ESEC/FSE} 2017},
pages     = {465--475},
publisher = {{ACM}},
year      = {2017},
url       = {https://doi.org/10.1145/3106237.3106259},
doi       = {10.1145/3106237.3106259},
timestamp = {Tue, 01 Feb 2022 10:45:16 +0100},
biburl    = {https://dblp.org/rec/conf/sigsoft/CedrimGMGSMFRC17.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zibran_real_time_search,
	author = {Zibran, Minhaz F. and Roy, Chanchal K.},
	title = {IDE-Based Real-Time Focused Search for near-Miss Clones},
	year = {2012},
	isbn = {9781450308571},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2245276.2231970},
	doi = {10.1145/2245276.2231970},
	abstract = {Code clone is a well-known code smell that needs to be detected and managed during the software development process. However, the existing clone detectors have one or more of the three shortcomings: (a) limitation in detecting Type-3 clones, (b) they come as stand-alone tools separate from IDE and thus cannot support clone-aware development, (c) they overwhelm the developer with all clones from the entire code-base, instead of a focused search for clones of a selected code segment of the developer's interest.This paper presents our IDE-integrated clone search tool, that addresses all the above issues. For clone detection, we adapt a suffix-tree-based hybrid algorithm. Through an asymptotic analysis, we show that our approach for clone detection is both time and memory efficient. Moreover, using three separate empirical studies, we demonstrate that our tool is flexibly usable for searching exact (Type-1) and near-miss (Type-2 and Type-3) clones with high precision and recall.},
	booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
	pages = {1235–1242},
	numpages = {8},
	keywords = {clone detection, clone search, maintenance, reengineering},
	location = {Trento, Italy},
	series = {SAC '12}
}

@inproceedings{Udding_Towards_Convenient_Management,
	author = {Uddin, Md Sharif and Roy, Chanchal K. and Schneider, Kevin A.},
	title = {Towards Convenient Management of Software Clone Codes in Practice: An Integrated Approach},
	year = {2015},
	publisher = {IBM Corp.},
	address = {USA},
	abstract = {Software code cloning is inevitable during software development and unmanaged cloning practice can create substantial problems for software maintenance and evolution. Current research in the area software clones includes, but is not limited to: finding ways to manage clones; gaining more control over clone generation; and, studying clone evolution and its effects on the evolution of software. In this study, we investigate tools and techniques for detecting, managing, and understanding the evolution of clones, as well as design a convenient tool to make those techniques available to a developer's software development environment. Towards the goal of promoting the practical use of code clone research and to provide better support for managing clones in software systems, we first developed SimEclipse: a clone-aware software development platform, and then, using the tool, we performed a study to investigate the usefulness of using a number clone based technologies in an integrated platform rather than using those discretely. Finally, a small scale user study is performed to evaluate SimEclipse's effectiveness, usability and information management with respect to some pre-defined clone management activities. We believe that both researchers and developers would enjoy and utilize the benefits of using SimEclipse for different aspects of code clone research as well as for managing cloned code in software systems.},
	booktitle = {Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering},
	pages = {211–220},
	numpages = {10},
	keywords = {IDE plugin, software clone, integrated clone management},
	location = {Markham, Canada},
	series = {CASCON '15}
}

@INPROCEEDINGS{InsightsSystemWideDuplication,
	author={Rieger, M. and Ducasse, S. and Lanza, M.},
	booktitle={11th
			Working Conference on Reverse Engineering},
	title={Insights into system-wide code
			duplication},
	year={2004},
	volume={},
	number={},
	pages={100-109},
	doi={10.1109/WCRE.2004.25}}

@inproceedings{SHINOBI,
author    = {Shinji Kawaguchi and
		Takanobu Yamashina and
		Hidetake Uwano and
		Kyohei Fushida and
		Yasutaka Kamei and
		Masataka Nagura and
		Hajimu Iida},
editor    = {Andy Zaidman and
Giuliano Antoniol and
St{\'{e}}phane Ducasse},
title     = {{SHINOBI:} {A} Tool for Automatic Code Clone Detection in the {IDE}},
booktitle = {16th Working Conference on Reverse Engineering, {WCRE} 2009, 13-16
		October 2009, Lille, France},
pages     = {313--314},
publisher = {{IEEE} Computer Society},
year      = {2009},
url       = {https://doi.org/10.1109/WCRE.2009.36},
doi       = {10.1109/WCRE.2009.36},
timestamp = {Wed, 16 Oct 2019 14:14:53 +0200},
biburl    = {https://dblp.org/rec/conf/wcre/KawaguchiYUFKNI99a.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ComparisonAndEvaluationOfTechniques,
author    = {Chanchal Kumar Roy and
		James R. Cordy and
		Rainer Koschke},
title     = {Comparison and evaluation of code clone detection techniques and tools:
{A} qualitative approach},
journal   = {Sci. Comput. Program.},
volume    = {74},
number    = {7},
pages     = {470--495},
year      = {2009},
url       = {https://doi.org/10.1016/j.scico.2009.02.007},
doi       = {10.1016/j.scico.2009.02.007},
timestamp = {Wed, 17 Feb 2021 21:56:05 +0100},
biburl    = {https://dblp.org/rec/journals/scp/RoyCK09.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BakerSparseDynamicProgramming,
	title = {Sparse Dynamic Programming for Longest Common Subsequence from Fragments},
	journal = {Journal of Algorithms},
	volume = {42},
	number = {2},
	pages = {231-254},
	year = {2002},
	issn = {0196-6774},
	doi = {https://doi.org/10.1006/jagm.2002.1214},
	url = {https://www.sciencedirect.com/science/article/pii/S0196677402912149},
	author = {Brenda S. Baker and Raffaele Giancarlo},
	abstract = {Sparse Dynamic Programming has emerged as an essential tool for the design of efficient algorithms for optimization problems coming from such diverse areas as computer science, computational biology, and speech recognition. We provide a new sparse dynamic programming technique that extends the Hunt–Szymanski paradigm for the computation of the longest common subsequence (LCS) and apply it to solve the LCS from Fragments problem: given a pair of strings X and Y (of length n and m, respectively) and a set M of matching substrings of X and Y, find the longest common subsequence based only on the symbol correspondences induced by the substrings. This problem arises in an application to analysis of software systems. Our algorithm solves the problem in O(|M|log|M|) time using balanced trees, or O(|M|loglogmin(|M|,nm/|M|)) time using Johnson's version of Flat Trees. These bounds apply for two cost measures. The algorithm can also be adapted to finding the usual LCS in O((m+n)log|Σ|+|M|log|M|) time using balanced trees or O((m+n)log|Σ|+|M|loglogmin(|M|,nm/|M|)) time using Johnson's version of Flat Trees, where M is the set of maximal matches between substrings of X and Y and Σ is the alphabet. These bounds improve on those of the original Hunt–Szymanski algorithm while retaining the overall approach.}
}

@book{crosby1980quality,
	title={Quality is Free: The Art of Making Quality Certain},
	author={Crosby, P.B.},
	isbn={9780451624680},
	lccn={79089296},
	url={https://books.google.no/books?id=3TMQt73LDooC},
	year={1980},
	publisher={New American Library}
}

@book{MetricsAndModelsInSoftwareQuality,
	author = {Kan, Stephen H.},
	title = {Metrics and Models in Software Quality Engineering},
	year = {2002},
	isbn = {0201729156},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	address = {USA},
	edition = {2nd},
}

@inproceedings{CloningByAccident,
	author    = {Raihan Al{-}Ekram and
			Cory Kapser and
			Richard C. Holt and
			Michael W. Godfrey},
	title     = {Cloning by accident: an empirical study of source code cloning across
			software systems},
	booktitle = {2005 International Symposium on Empirical Software Engineering {(ISESE}
			2005), 17-18 November 2005, Noosa Heads, Australia},
	pages     = {376--385},
	publisher = {{IEEE} Computer Society},
	year      = {2005},
	url       = {https://doi.org/10.1109/ISESE.2005.1541846},
	doi       = {10.1109/ISESE.2005.1541846},
	timestamp = {Wed, 16 Oct 2019 14:14:56 +0200},
	biburl    = {https://dblp.org/rec/conf/isese/Al-EkramKHG05.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{treesitter,
	title = {Tree-sitter},
	author = {Max Brunsfeld},
	howpublished = {\url{https://tree-sitter.github.io/tree-sitter/}},
	note = {Accessed: 2022-04-16}
}

@misc{lsp,
	title = {Language Server Protocol},
	author = {Microsoft},
	howpublished = {\url{https://microsoft.github.io/language-server-protocol/}},
	note = {Accessed: 2023-02-17}
}


@article{IncrementalParsing,
	author    = {Carlo Ghezzi and
			Dino Mandrioli},
	title     = {Incremental Parsing},
	journal   = {{ACM} Trans. Program. Lang. Syst.},
	volume    = {1},
	number    = {1},
	pages     = {58--70},
	year      = {1979},
	url       = {https://doi.org/10.1145/357062.357066},
	doi       = {10.1145/357062.357066},
	timestamp = {Tue, 06 Nov 2018 12:51:29 +0100},
	biburl    = {https://dblp.org/rec/journals/toplas/GhezziM79.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{PracticalAlgorithmsForIncremental,
	Author = {Wagner, Tim A.},
	Title = {Practical Algorithms for Incremental Software Development Environments},
	School = {EECS Department, University of California, Berkeley},
	Year = {1998},
	Month = {Mar},
	URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/1998/5885.html},
	Number = {UCB/CSD-97-946},
	Abstract = {We describe an integrated collection of algorithms and data structures to serve as the basis for a practical incremental software development environment. A self-versioning representation provides a uniform model that embraces both natural and programming language documents in a single, consistent framework. Software artifacts in this representation provide fine-grained change reports to all tools in the environment. We then present algorithms for the initial construction and subsequent maintenance of persistent, structured documents that support unrestricted user editing. These algorithms possess several novel aspects: they are more general than previous approaches, address issues of practical importance, including scalability and information preservation, and are optimal in both space and time. Since deterministic parsing is too restrictive a model to describe some common programming languages, we also investigate support for multiple structural interpretations: incremental non-deterministic parsing is used to construct a compact form that efficiently encodes syntactic ambiguity. Later analyses may resolve ambiguous phrases through syntactic or semantic disambiguation. This result provides the first known method for handling C, C++, COBOL, and FORTRAN in an incremental framework derived from formal specifications. <p>Our transformation and analysis algorithms are designed to avoid spurious changes, which result in lost information and unnecessary recomputation by later stages. We provide the first non-operational definition of optimal node reuse in the context of incremental parsing, and present optimal algorithms for retaining tokens and nodes during incremental lexing and parsing. We also exploit the tight integration between versioning and incremental analysis to provide a novel history-sensitive approach to error handling. Our error recovery mechanism reports problems in terms of the user's own changes in a language-independent, non-correcting, automated, and fully incremental manner. <p>This work can be read at several levels: as a refinement and extension of previous results to address issues of scalability, end-to-end performance, generality, and description reuse; as a 'cookbook' for constructing the framework of a practical incremental environment into which semantic analysis, code generation, presentation, and other services can be plugged; and as a set of separate (but interoperable) solutions to open problems in the analysis and representation of software artifacts. Our results are promising: in addition to embracing a realistic language model, both asymptotic and empirical measurements demonstrate that we have overcome barriers in performance and scalability. Incremental methods can now be applied to commercially important languages, and may finally become the standard approach to constructing language-based tools and services.}
}

@INPROCEEDINGS{IndexBasedIncrementalCloneDetection,
	author={Hummel, Benjamin and Juergens, Elmar and Heinemann, Lars and Conradt, Michael},
	booktitle={2010 IEEE International Conference on Software Maintenance},
	title={Index-based code clone detection: incremental, distributed, scalable},
	year={2010},
	volume={},
	number={},
	pages={1-9},
	doi={10.1109/ICSM.2010.5609665}
}

@INPROCEEDINGS{GodeIncrementalCloneDetection,
	author={Göde, Nils and Koschke, Rainer},
	booktitle={2009 13th European Conference on Software Maintenance and Reengineering},
	title={Incremental Clone Detection},
	year={2009},
	volume={},
	number={},
	pages={219-228},
	doi={10.1109/CSMR.2009.20}
}


@article{SeedType4Detection,
	author    = {Zhipeng Xue and
			Haoran Liu and
			Chen Zeng and
			Chenglong Zhou and
			Xiaodong Liu and
			Yangtao Ge and
			JinJing Zhao},
	title     = {{SEED:} Semantic Graph based Deep detection for type-4 clone},
	journal   = {CoRR},
	volume    = {abs/2109.12079},
	year      = {2021},
	url       = {https://arxiv.org/abs/2109.12079},
	eprinttype = {arXiv},
	eprint    = {2109.12079},
	timestamp = {Mon, 27 Sep 2021 15:21:05 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2109-12079.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{LocalitySensitiveHashingIncremental,
	author    = {Tung Thanh Nguyen and
			Hoan Anh Nguyen and
			Jafar M. Al{-}Kofahi and
			Nam H. Pham and
			Tien N. Nguyen},
	title     = {Scalable and incremental clone detection for evolving software},
	booktitle = {25th {IEEE} International Conference on Software Maintenance {(ICSM}
			2009), September 20-26, 2009, Edmonton, Alberta, Canada},
	pages     = {491--494},
	publisher = {{IEEE} Computer Society},
	year      = {2009},
	url       = {https://doi.org/10.1109/ICSM.2009.5306283},
	doi       = {10.1109/ICSM.2009.5306283},
	timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
	biburl    = {https://dblp.org/rec/conf/icsm/NguyenNAPN09.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{SiameseScalableAndIncrementalClone,
	author    = {Chaiyong Ragkhitwetsagul and
			Jens Krinke},
	title     = {Siamese: scalable and incremental code clone search via multiple code
			representations},
	journal   = {Empir. Softw. Eng.},
	volume    = {24},
	number    = {4},
	pages     = {2236--2284},
	year      = {2019},
	url       = {https://doi.org/10.1007/s10664-019-09697-7},
	doi       = {10.1007/s10664-019-09697-7},
	timestamp = {Tue, 25 Aug 2020 16:58:46 +0200},
	biburl    = {https://dblp.org/rec/journals/ese/Ragkhitwetsagul19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{CCLearner,
author    = {Liuqing Li and
		He Feng and
		Wenjie Zhuang and
		Na Meng and
		Barbara G. Ryder},
title     = {CCLearner: {A} Deep Learning-Based Clone Detection Approach},
booktitle = {2017 {IEEE} International Conference on Software Maintenance and Evolution,
		{ICSME} 2017, Shanghai, China, September 17-22, 2017},
pages     = {249--260},
publisher = {{IEEE} Computer Society},
year      = {2017},
url       = {https://doi.org/10.1109/ICSME.2017.46},
doi       = {10.1109/ICSME.2017.46},
timestamp = {Mon, 23 Aug 2021 15:53:56 +0200},
biburl    = {https://dblp.org/rec/conf/icsm/LiFZMR17.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{TechnicalDebt,
	author    = {Paris Avgeriou and
			Philippe Kruchten and
			Ipek Ozkaya and
			Carolyn B. Seaman},
	title     = {Managing Technical Debt in Software Engineering (Dagstuhl Seminar
			16162)},
	journal   = {Dagstuhl Reports},
	volume    = {6},
	number    = {4},
	pages     = {110--138},
	year      = {2016},
	url       = {https://doi.org/10.4230/DagRep.6.4.110},
	doi       = {10.4230/DagRep.6.4.110},
	timestamp = {Wed, 07 Jun 2017 14:40:01 +0200},
	biburl    = {https://dblp.org/rec/journals/dagstuhl-reports/AvgeriouKOS16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{SuffixArrayConstruction,
author="K{\"a}rkk{\"a}inen, Juha",
editor="Kao, Ming-Yang",
title="Suffix Array Construction",
bookTitle="Encyclopedia of Algorithms",
year="2016",
publisher="Springer New York",
address="New York, NY",
pages="2141--2144",
isbn="978-1-4939-2864-4",
doi="10.1007/978-1-4939-2864-4_412",
url="https://doi.org/10.1007/978-1-4939-2864-4_412"
}
@article{LinearTimeSuffixArraySAIS,
	author    = {Ge Nong and
			Sen Zhang and
			Wai Hong Chan},
	title     = {Two Efficient Algorithms for Linear Time Suffix Array Construction},
	journal   = {{IEEE} Trans. Computers},
	volume    = {60},
	number    = {10},
	pages     = {1471--1484},
	year      = {2011},
	url       = {https://doi.org/10.1109/TC.2010.188},
	doi       = {10.1109/TC.2010.188},
	timestamp = {Sat, 20 May 2017 00:24:37 +0200},
	biburl    = {https://dblp.org/rec/journals/tc/NongZC11.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Ukkonen,
	title={On-line construction of suffix trees},
	author={Ukkonen, Esko},
	journal={Algorithmica},
	volume={14},
	number={3},
	pages={249--260},
	year={1995},
	publisher={Springer}
}


@INPROCEEDINGS{Deckard,
	author={Jiang, Lingxiao and Misherghi, Ghassan and Su, Zhendong and Glondu, Stephane},
	booktitle={29th International Conference on Software Engineering (ICSE'07)},
	title={DECKARD: Scalable and Accurate Tree-Based Detection of Code Clones},
	year={2007},
	volume={},
	number={},
	pages={96-105},
	doi={10.1109/ICSE.2007.30}
}


@inproceedings{ASTCloneDetection,
	author = {Baxter, Ira and Yahin, Andrew and de Moura, Leonardo and Sant'Anna, Marcelo and Bier, Lorraine},
	year = {1998},
	month = {01},
	pages = {368-377},
	title = {Clone Detection Using Abstract Syntax Trees.},
	volume = {368-377},
	journal = {Proc. of International Conference on Software Maintenance},
	doi = {10.1109/ICSM.1998.738528}
}


@INPROCEEDINGS{JacobsonsRank,
	author={Jacobson, G.},
	booktitle={30th Annual Symposium on Foundations of Computer Science},
	title={Space-efficient static trees and graphs},
	year={1989},
	volume={},
	number={},
	pages={549-554},
	doi={10.1109/SFCS.1989.63533}
}

@article{ReplaceSuffixTreeWithEnchancedSuffixArray,
	title = {Replacing suffix trees with enhanced suffix arrays},
	journal = {Journal of Discrete Algorithms},
	volume = {2},
	number = {1},
	pages = {53-86},
	year = {2004},
	note = {The 9th International Symposium on String Processing and Information Retrieval},
	issn = {1570-8667},
	doi = {https://doi.org/10.1016/S1570-8667(03)00065-0},
	url = {https://www.sciencedirect.com/science/article/pii/S1570866703000650},
	author = {Mohamed Ibrahim Abouelhoda and Stefan Kurtz and Enno Ohlebusch},
	keywords = {Suffix array, Suffix tree, Repeat analysis, Genome comparison, Pattern matching},
	abstract = {The suffix tree is one of the most important data structures in string processing and comparative genomics. However, the space consumption of the suffix tree is a bottleneck in large scale applications such as genome analysis. In this article, we will overcome this obstacle. We will show how every algorithm that uses a suffix tree as data structure can systematically be replaced with an algorithm that uses an enhanced suffix array and solves the same problem in the same time complexity. The generic name enhanced suffix array stands for data structures consisting of the suffix array and additional tables. Our new algorithms are not only more space efficient than previous ones, but they are also faster and easier to implement.}
}

@inproceedings{BWT,
	title={A Block-sorting Lossless Data Compression Algorithm},
	author={Michael Burrows and David J. Wheeler},
	year={1994}
}

@article{WagnerFischer,
	author = {Wagner, Robert A. and Fischer, Michael J.},
	title = {The String-to-String Correction Problem},
	year = {1974},
	issue_date = {Jan. 1974},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {21},
	number = {1},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/321796.321811},
	doi = {10.1145/321796.321811},
	abstract = {The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.},
	journal = {J. ACM},
	month = {jan},
	pages = {168–173},
	numpages = {6}
}


@book{GangOfFour,
	added-at = {2010-06-05T16:40:25.000+0200},
	asin = {0201633612},
	author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John M.},
	biburl = {https://www.bibsonomy.org/bibtex/27e3f1154ab1fbce54752a46dba7f2217/pnk},
	description = {Design Patterns: Elements of Reusable Object-Oriented Software (9780201633610): Erich Gamma, Richard Helm, Ralph Johnson, John M. Vlissides: Books},
	dewey = {005.12},
	ean = {9780201633610},
	edition = 1,
	interhash = {7fe32957be97afaf4ecb38b5490d23b4},
	intrahash = {7e3f1154ab1fbce54752a46dba7f2217},
	isbn = {0201633612},
	keywords = {DBIS Design Object-Oriented Patterns SS2010 Seminar Software},
	publisher = {Addison-Wesley Professional},
	timestamp = {2010-06-05T16:40:25.000+0200},
	title = {Design Patterns: Elements of Reusable Object-Oriented Software},
	url = {http://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612/ref=ntt_at_ep_dpi_1},
	year = 1994
}

@InProceedings{UkkonenEditDistance,
	author="Ukkonen, Esko",
	editor="Karpinski, Marek",
	title="On approximate string matching",
	booktitle="Foundations of Computation Theory",
	year="1983",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="487--495",
	abstract="An algorithm is given for computing the edit distance as well as the corresponding sequence of editing steps (insertions, deletions, changes, transpositions of adjacent symbols) between two strings a1a2...am and b1b2...bn. The algorithm needs time 0(s{\textperiodcentered}min(m,n)) and space 0(s2) where s is the edit distance, that is, the minimum number of editing steps needed to transform a1a2...am to b1b2...bn. For small s this is a considerable improvement over the best previously known algorithm that needs time and space 0(mn). If the editing sequence is not required, the space complexity of our algorithm reduces to 0(s). Given a threshold value t, the algorithm can also be modified to test in time 0(t{\textperiodcentered}min(m,n)) and space 0(t) whether the edit distance of the two strings is at most t.",
	isbn="978-3-540-38682-7"
}

@article{HirschbergsAlgorithm,
	author = {Hirschberg, D. S.},
	title = {A Linear Space Algorithm for Computing Maximal Common Subsequences},
	year = {1975},
	issue_date = {June 1975},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {18},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/360825.360861},
	doi = {10.1145/360825.360861},
	abstract = {The problem of finding a longest common subsequence of two strings has been solved in quadratic time and space. An algorithm is presented which will solve this problem in quadratic time and in linear space.},
	journal = {Commun. ACM},
	month = {jun},
	pages = {341–343},
	numpages = {3},
	keywords = {string correction, editing, subsequence, longest common sequence}
}

@inproceedings{WaveletTree,
  author    = {Roberto Grossi and
               Ankur Gupta and
               Jeffrey Scott Vitter},
  title     = {High-order entropy-compressed text indexes},
  booktitle = {Proceedings of the Fourteenth Annual {ACM-SIAM} Symposium on Discrete
               Algorithms, January 12-14, 2003, Baltimore, Maryland, {USA}},
  pages     = {841--850},
  publisher = {{ACM/SIAM}},
  year      = {2003},
  url       = {http://dl.acm.org/citation.cfm?id=644108.644250},
  timestamp = {Fri, 02 Dec 2016 10:11:18 +0100},
  biburl    = {https://dblp.org/rec/conf/soda/GrossiGV03.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{WaveletMatrix,
  author    = {Francisco Claude and
               Gonzalo Navarro and
               Alberto Ord{\'{o}}{\~{n}}ez Pereira},
  title     = {The wavelet matrix: An efficient wavelet tree for large alphabets},
  journal   = {Inf. Syst.},
  volume    = {47},
  pages     = {15--32},
  year      = {2015},
  url       = {https://doi.org/10.1016/j.is.2014.06.002},
  doi       = {10.1016/j.is.2014.06.002},
  timestamp = {Thu, 28 Dec 2017 16:11:05 +0100},
  biburl    = {https://dblp.org/rec/journals/is/ClaudeNP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{LevelwiseWaveletTree,
    title = {Rank and select revisited and extended},
    journal = {Theoretical Computer Science},
    volume = {387},
    number = {3},
    pages = {332-347},
    year = {2007},
    note = {The Burrows-Wheeler Transform},
    issn = {0304-3975},
    doi = {https://doi.org/10.1016/j.tcs.2007.07.013},
    url = {https://www.sciencedirect.com/science/article/pii/S0304397507005300},
    author = {Veli Mäkinen and Gonzalo Navarro},
    keywords = {Succinct data structures, Compressed data structures, Gap encoding, Range searching, Position-restricted substring searching, Wavelet trees, Substring rank and select},
    abstract = {The deep connection between the Burrows–Wheeler transform (BWT) and the so-called rank and select data structures for symbol sequences is the basis of most successful approaches to compressed text indexing. Rank of a symbol at a given position equals the number of times the symbol appears in the corresponding prefix of the sequence. Select is the inverse, retrieving the positions of the symbol occurrences. It has been shown that improvements to rank/select algorithms, in combination with the BWT, turn into improved compressed text indexes. This paper is devoted to alternative implementations and extensions of rank and select data structures. First, we show that one can use gap encoding techniques to obtain constant time rank and select queries in essentially the same space as what is achieved by the best current direct solution (and sometimes less). Second, we extend symbol rank and select to substring rank and select, giving several space/time trade-offs for the problem. An application of these queries is in position-restricted substring searching, where one can specify the range in the text where the search is restricted to, and only occurrences residing in that range are to be reported. In addition, arbitrary occurrences are reported in text position order. Several byproducts of our results display connections with searchable partial sums, Chazelle’s two-dimensional data structures, and Grossi et al.’s wavelet trees.}
}

@book{CLRS,
    author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
    title = {Introduction to Algorithms, Third Edition},
    year = {2009},
    isbn = {0262033844},
    publisher = {The MIT Press},
    edition = {3rd},
    abstract = {If you had to buy just one text on algorithms, Introduction to Algorithms is a magnificent choice. The book begins by considering the mathematical foundations of the analysis of algorithms and maintains this mathematical rigor throughout the work. The tools developed in these opening sections are then applied to sorting, data structures, graphs, and a variety of selected algorithms including computational geometry, string algorithms, parallel models of computation, fast Fourier transforms (FFTs), and more. This book's strength lies in its encyclopedic range, clear exposition, and powerful analysis. Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book is a great resource on the basic tools used to analyze the performance of algorithms.}
}

@InProceedings{GLR,
	author="Lang, Bernard",
	editor="Loeckx, Jacques",
	title="Deterministic Techniques for Efficient Non-Deterministic Parsers",
	booktitle="Automata, Languages and Programming",
	year="1974",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="255--269",
	abstract="A general study of parallel non-deterministic parsing and translation {\`a} la Earley is developped formally, based on non-deterministic pushdown acceptor-transducers. Several results (camplexity and efficiency) are established, same new and other previously proved only in special cases. As an application, we show that for every family of deterministic context-free pushdown parsers (e.g. precedence, LR(k), LL(k), ...) there is a family of general context-free parallel parsers that have the same efficiency in most practical cases (e.g. analysis of programming languages).",
	isbn="978-3-662-21545-6"
}


@article{Bakerdup,
  title={A program for identifying duplicated code},
  author={Baker, Brenda S},
  journal={Computing Science and Statistics},
  pages={49--49},
  year={1993},
  publisher={Citeseer}
}

@article{BakerLCS,
    author = {Baker, Brenda and Giancarlo, Raffaele},
    year = {2002},
    month = {02},
    pages = {231-254},
    title = {Sparse Dynamic Programming for Longest Common Subsequence from Fragments},
    volume = {42},
    journal = {Journal of Algorithms},
    doi = {10.1006/jagm.2002.1214}
}
